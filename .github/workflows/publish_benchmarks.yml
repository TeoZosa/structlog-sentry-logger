---
name: Publish Performance Testing Benchmarks
on:  # yamllint disable-line rule:truthy
  workflow_run:
    workflows:
      # Upstream workflow runs on pushes and always completes;
      # needed because Dependabot-initiated pushes to master do not have access
      # to repository secrets on which this workflow relies.
      # see: dependabot/dependabot-core/issues/3253
      - Dependabot Push Check
    types:
      - completed

jobs:
  build-and-publish-benchmarks:
    strategy:
      matrix:
        os: ["ubuntu-latest", "windows-latest", "macos-latest"]
        python-version: ["3.7", "3.8", "3.9"]
        library-type: ["pure_python", "c_library"]

    name: Performance testing for Python ${{ matrix.python-version }} (${{ matrix.os }}) [${{ matrix.library-type }}]
    runs-on: ${{ matrix.os }}

    steps:
      - name: Check out the repository
        uses: actions/checkout@v2.4.0
        with:
          ref: ${{ github.event.workflow_run.head_branch }}

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          # yamllint disable-line rule:braces
          python-version: ${{ matrix.python-version }}

      - name: Install Poetry
        run: |
          pip install --constraint=.github/workflows/constraints.txt poetry
          poetry --version

      - name: Configure Poetry
        run: |
          poetry config cache-dir "${GITHUB_WORKSPACE}/.cache/pypoetry"
          poetry config virtualenvs.in-project true
          poetry config --list

      - name: Install Tox
        run: |
          pip install --constraint=.github/workflows/constraints.txt tox
          tox --version

      - name: Load cached tox testenv(s) (if they exist)
        id: cached-poetry-dependencies
        uses: actions/cache@v2
        with:
          path: |
            .tox
          key: tox-${{ github.workflow }}-${{ github.job }}-${{ runner.os }}-CPython${{ matrix.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: Run benchmarks for ${{ matrix.python-version }}
        id: performance-testing
        uses: nick-invision/retry@v2
        with:
          timeout_minutes: 10
          max_attempts: 3
          shell: bash
          command: |
            TOX_COMPATIBLE_VERSION=$(echo ${{ matrix.python-version }} | tr -d '.')
            TOX_TESTENV="py${TOX_COMPATIBLE_VERSION}-benchmark-${{ matrix.library-type }}"
            make "tox-${TOX_TESTENV}"

            BENCHMARK_FILE=$(find .benchmarks -maxdepth 2 -type f -name "*.json" | sort  --version-sort --reverse | tail -n 1)
            echo "::set-output name=benchmark_file_path::${BENCHMARK_FILE}"

            ENV_DIR_NAME=$(basename "${BENCHMARK_FILE%/*}" | tr '-' '/')
            NAMESPACED_TARGET_OUTPUT_DIR="${ENV_DIR_NAME}/${{ matrix.library-type }}"
            echo "::set-output name=target_output_dir::${NAMESPACED_TARGET_OUTPUT_DIR}"

      - name: Store the environment-specific benchmarks
        uses: actions/upload-artifact@v2.3.1
        with:
          name: benchmark-data-files
          path: .benchmarks

      - name: Reset Poetry lockfile for Checkout
        run: |
          git checkout poetry.lock || echo "Skipping step"

      - name: Publish benchmarks to GH pages
        uses: TeoZosa/github-action-benchmark@v1.8.2
        if: github.event.workflow_run.head_branch == 'master'
        with:
          tool: 'pytest'
          benchmark-data-dir-path: dev/bench/${{ steps.performance-testing.outputs.target_output_dir }}
          # yamllint disable-line rule:braces
          output-file-path: ${{ steps.performance-testing.outputs.benchmark_file_path }}
          # yamllint disable-line rule:braces
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Push and deploy GitHub pages branch automatically
          auto-push: true

  # Plot aggregate benchmarks table ----------------------
  plot-benchmarks-table:
    name: Plot aggregated benchmarks table
    runs-on: ubuntu-latest
    needs:
      - build-and-publish-benchmarks
    env:
      BENCHMARK_OPTS: "--name=normal --sort=name --group-by=name --histogram"
      PY_COLORS: 1
    steps:
      - name: Check out the repository
        uses: actions/checkout@v2.4.0
        with:
          ref: ${{ github.event.workflow_run.head_branch }}

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: "3.9"

      - name: Upgrade pip
        run: |
          pip install --constraint=.github/workflows/constraints.txt pip
          pip --version

      - name: Install pytest-benchmark
        run: |
          pip install pytest-benchmark[histogram]

      - name: Download the benchmarks
        uses: actions/download-artifact@v2.1.0
        with:
          name: benchmark-data-files
          path: .benchmarks

      - name: List benchmark data files
        run: |
          find .benchmarks

      - name: Plot aggregate results tables (by Python interpreter)
        run: |
          # Disable '\n' field splitting for proper downstream command substitutions
          IFS=' '
          BENCHMARK_DATA_DIRS=$(ls .benchmarks | grep -v "^.*bit$" | sed 's/\(.*\)/\1\/*/')

          echo "**By Python versions**"
          for MINOR_VER in 7 8 9;
          do
            PY_VER=Cpython3.${MINOR_VER}
            PY_VER_BENCH_DIRS=$(echo ${BENCHMARK_DATA_DIRS} | grep "^.*${PY_VER}.*$" | tr '\n' ' ')
            echo "${PY_VER} benchmark data directories: ${PY_VER_BENCH_DIRS}"

            pytest-benchmark compare ${PY_VER_BENCH_DIRS} ${{ env.BENCHMARK_OPTS }}
          done

      - name: Plot aggregate results tables (by platform)
        run: |
          # Disable '\n' field splitting for proper downstream command substitutions
          IFS=' '
          BENCHMARK_DATA_DIRS=$(ls .benchmarks | grep -v "^.*bit$" | sed 's/\(.*\)/\1\/*/')

          for SYS_PLATFORM in Linux Darwin Win32;
          do
            SYS_PLATFORM_BENCH_DIRS=$(echo ${BENCHMARK_DATA_DIRS} | grep  "^${SYS_PLATFORM}.*$" | tr '\n' ' ')
            echo "${SYS_PLATFORM} benchmark data directories: ${SYS_PLATFORM_BENCH_DIRS}"

            pytest-benchmark compare ${SYS_PLATFORM_BENCH_DIRS} ${{ env.BENCHMARK_OPTS }}
          done

      - name: Plot aggregate results tables (platform x Python interpreter)
        run: |
          # Disable '\n' field splitting for proper downstream command substitutions
          IFS=' '
          BENCHMARK_DATA_DIRS=$(ls .benchmarks | grep -v "^.*bit$" | sed 's/\(.*\)/\1\/*/')

          echo "**Pure Python vs C Library (platform x Python interpreter)**"
          for DIR in $(echo ${BENCHMARK_DATA_DIRS} | tr '\n' ' ');
          do
            echo "Platform x Python intepreter: ${DIR}"
            pytest-benchmark compare ${DIR} ${{ env.BENCHMARK_OPTS }}
          done

      - name: Plot aggregate results tables (by library type)
        run: |
          # Disable '\n' field splitting for proper downstream command substitutions
          IFS=' '
          BENCHMARK_DATA_DIRS=$(ls .benchmarks | grep -v "^.*bit$" | sed 's/\(.*\)/\1\/*/')

          echo "**Pure Python (All)**"
          pytest-benchmark compare pure_python ${{ env.BENCHMARK_OPTS }}

          echo "**C Library**"
          pytest-benchmark compare c_library ${{ env.BENCHMARK_OPTS }}

          echo "**Pure Python vs. C library (Aggregate)**"
          pytest-benchmark compare pure_python c_library ${{ env.BENCHMARK_OPTS }}
